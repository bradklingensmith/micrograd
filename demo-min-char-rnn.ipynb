{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b1ab417",
   "metadata": {},
   "source": [
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy). Adapted to micrograd by @bradklingensmith\n",
    "BSD License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f76066",
   "metadata": {},
   "source": [
    "# Data and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27baac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 2357 characters, 50 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "def remove_comments(data): # This makes the data easier to learn\n",
    "    return '\\n'.join([(l[:l.index('#')] if '#' in l else l).rstrip() for l in data.splitlines()])\n",
    "data = remove_comments(open('micrograd/engine.py', 'r').read()) # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "data_char_ixs = [char_to_ix[ch] for ch in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0def43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 16 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cdfab0",
   "metadata": {},
   "source": [
    "# All the helpers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5cb9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from micrograd.engine import Value\n",
    "import micrograd.nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def make_unary(f, df, name):\n",
    "    def unary(x):\n",
    "        out = Value(f(x.data), (x,), name)\n",
    "\n",
    "        def _backward():\n",
    "            x.grad += df(x.data) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    return unary\n",
    "\n",
    "tanh = make_unary(math.tanh, lambda x: 1. - math.tanh(x)**2., 'tanh')\n",
    "exp = make_unary(math.exp, math.exp, 'exp')\n",
    "\n",
    "def softmax(xs):\n",
    "    max_x = max([x.data for x in xs]) # for numerical stability, subtract off the max\n",
    "    zs = [x - max_x for x in xs]\n",
    "\n",
    "    exp_zs = list(map(exp, zs))\n",
    "    sum_exp_zs = sum(exp_zs)\n",
    "    ps = [z / sum_exp_zs for z in exp_zs] # probabilies\n",
    "    return ps\n",
    "\n",
    "def softmax_cross_entropy(xs, target_i):\n",
    "    max_x = max(x.data for x in xs) # for numerical stability, subtract off the max\n",
    "    exp_xs = list(map(math.exp, (x.data - max_x for x in xs)))\n",
    "    sum_exp_xs = sum(exp_xs)\n",
    "    ps = [exp_x / sum_exp_xs for exp_x in exp_xs] # probabilies\n",
    "    loss = -math.log(ps[target_i])\n",
    "\n",
    "    out = Value(loss, xs, 'softmax cross entropy')\n",
    "    def backward():\n",
    "        for i, x in enumerate(xs):\n",
    "            x.grad += ps[i] - (i == target_i)\n",
    "    out._backward = backward\n",
    "    return out\n",
    "\n",
    "def one_hot(index, length):  # encode in 1-of-k representation\n",
    "    return [0.] * index + [1.] + [0.] * (length - index - 1)\n",
    "\n",
    "def lerp(a, b, fraction):\n",
    "    return a + (b - a) * fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029ce82",
   "metadata": {},
   "source": [
    "# The Recurrent Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d5f1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN of [RNNLayer of [TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16), TanhRNNNeuron(50, 16)], RNNLayer of [LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0), LinearRNNNeuron(16, 0)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNNNeuron(micrograd.nn.Module):\n",
    "    ''' Takes x (prev layer output) and h (previous hidden state) as \n",
    "    input and computes dot(xs, wxhs) + dot(hs, whhs) + b'''\n",
    "\n",
    "    def __init__(self, nin, nh, recurrent=True, nonlin=True):\n",
    "        ''' if recurrent is False, this will act like a plain neuron '''\n",
    "        self.wxs = [Value(random.gauss(0.,1.)) for _ in range(nin)]\n",
    "        self.whs = [Value(random.gauss(0.,1.)) for _ in range(nh)] if recurrent else []\n",
    "        self.b = Value(0.)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, xs, hs):\n",
    "        data = lambda x: x.data if type(x) == Value else x\n",
    "        act = Value(0., [v for v in self.parameters() + xs + hs if type(v) == Value], 'RNNNeuron')\n",
    "        act.data = sum((w.data*data(x) for w, x in zip(self.wxs+self.whs, xs+hs)), self.b.data)\n",
    "        \n",
    "        def backward():\n",
    "            for w, x in zip(self.wxs + self.whs, xs + hs):\n",
    "                w.grad += data(x) * act.grad\n",
    "                if type(x) == Value:\n",
    "                    x.grad += w.data * act.grad\n",
    "            self.b.grad += act.grad\n",
    "        act._backward = backward\n",
    "            \n",
    "        return tanh(act) if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.wxs + self.whs + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'Tanh' if self.nonlin else 'Linear'}RNNNeuron({len(self.wxs)}, {len(self.whs)})\"\n",
    "\n",
    "class RNNLayer(micrograd.nn.Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [RNNNeuron(nin, nout, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return [0.] * len(self.neurons)\n",
    "\n",
    "    def __call__(self, xs, hs):\n",
    "        return [n(xs, hs) for n in self.neurons]\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"RNNLayer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "\n",
    "class RNN(micrograd.nn.Module):\n",
    "    \"\"\" A network with a sequence of RNNLayer\n",
    "    \n",
    "    Note that the last layer is not recurrent and acts like a plain Layer.\"\"\"\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        def make_layer(i):\n",
    "            is_not_last = i<len(nouts)-1\n",
    "            return RNNLayer(sz[i], sz[i+1], recurrent=is_not_last, nonlin=is_not_last)\n",
    "        self.layers = list(map(make_layer, range(len(nouts))))\n",
    "\n",
    "    def __call__(self, x, prev_hiddens=None):\n",
    "        prev_hiddens = prev_hiddens if prev_hiddens else []\n",
    "        hiddens = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, prev_hiddens[i] if len(prev_hiddens) > i else layer.init_hidden())\n",
    "            hiddens.append(x)\n",
    "        return x, hiddens\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"RNN of [{', '.join(str(layer) for layer in self.layers)}]\"\n",
    "    \n",
    "rnn = RNN(vocab_size, [hidden_size, vocab_size])\n",
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63834113",
   "metadata": {},
   "source": [
    "# Sampling (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c54f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ixs(hs, ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of character indices from the model\n",
    "    hs is memory state, ix is seed character index for first time step\n",
    "    \"\"\"\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        ys, hs = rnn(one_hot(ix, vocab_size), hs)\n",
    "        ix = random.choices(range(vocab_size), weights=[p.data**2. for p in softmax(ys)])[0]\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def sample_text(hs, seed_char, n):\n",
    "    ixs = sample_ixs(hs, char_to_ix[seed_char], n)\n",
    "    return ''.join(ix_to_char[ix] for ix in ixs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53f562",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a4dce",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfe2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW:\n",
    "    def __init__(self, params):\n",
    "        self.params, self.m, self.v, self.t = params, [0.]*len(params), [0.]*len(params), 0\n",
    "        \n",
    "    def update_params(self, lr=0.005, beta1=0.9, beta2=0.999, weight_decay=1e-2):\n",
    "        self.t += 1\n",
    "        for i, param in enumerate(self.params):\n",
    "            self.m[i] = lerp(param.grad, self.m[i], beta1) # momentum\n",
    "            self.v[i] = lerp(param.grad**2., self.v[i], beta2) # grad magnitudes\n",
    "            \n",
    "            m = self.m[i] / (1. - beta1**self.t)\n",
    "            v = self.v[i] / (1. - beta2**self.t)            \n",
    "            param.data -= lr * (m / math.sqrt(v + 1e-8) + weight_decay * param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1ad69",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae99be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(inputs, target_is, hs=None):\n",
    "    \"\"\"\n",
    "    inputs, target_is are both list of character indices.\n",
    "    hs is the initial hidden state (list of hidden_size floats or None)\n",
    "    returns the loss and last hidden state\n",
    "    \"\"\"\n",
    "    loss = 0.\n",
    "    # forward pass\n",
    "    for input_t, target_i in zip(inputs, target_is):\n",
    "        xs = one_hot(input_t, vocab_size)\n",
    "        ys, hs = rnn(xs, hs)\n",
    "        loss += softmax_cross_entropy(ys, target_i)\n",
    "    loss /= len(inputs) # avg over sequence so loss/gradients are sequence-length-independent\n",
    "    hs = [[h.data for h in hs[0]]] # make plain data copy to break _backward chain\n",
    "    return loss, hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112662bf",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffca831e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      " +luo0k<e]\n",
      "p{{]{]1\n",
      " gg(<*(u_*]y(\n",
      "tom.>L_*].(ytu>..ue(fn(fn(fn(fn(f/(\n",
      "hud*.:o0k<0>y(]\n",
      "pVViN*.:*.do0k<e]\n",
      "\"{\"p{]1h1><e]\n",
      "p{{p1nh \"]Uyec]0>yh1<e.y*.:*.do.mo*f<.vu*11<e>=>yn*1<*(Loe\"mNt*>L_p1<e>=]moe]<+,u*(y \n",
      "----\n",
      "iter 1000, loss: 3.4395131818633593      \n",
      "----\n",
      " he                                                                                                                                                                                                       \n",
      "----\n",
      "iter 2000, loss: 2.623248002667137       \n",
      "----\n",
      " ,                                                                                                                            \n",
      "                                                                           \n",
      "----\n",
      "iter 3000, loss: 2.1513223763664278      \n",
      "----\n",
      " = self._e__t)\n",
      "                                                                   def __p(iederther.d warself.__a_ethers_(self.:\n",
      "                                                                         \n",
      "----\n",
      "iter 4000, loss: 1.8431953319782353      \n",
      "----\n",
      "                  oueself.grad(seth.,\n",
      "w routkwald (uetother):\n",
      "                            outa_(self.,                                     oueself,                                                       \n",
      "----\n",
      "iter 5000, loss: 1.6430364312108197      \n",
      "----\n",
      " lf.d waldacoabacritself, 'f.grachelf _bacta d wan                    out.daca d wald\n",
      "                                        other.:\n",
      "                                                    other):\n",
      "        \n",
      "----\n",
      "iter 6000, loss: 1.4977183380085775      \n",
      "----\n",
      " rac (self, other =                                                                                                          otother)\n",
      "                af __self.ser   warsen           other):\n",
      "           \n",
      "----\n",
      "iter 7000, loss: 1.38039396023956        \n",
      "----\n",
      " f __self, otherutatackward(self.grackwar_backward(self, other):\n",
      "                     return self, other):\n",
      "          rela  __rp(self, otred(self, other):\n",
      "                                 other):\n",
      "       \n",
      "----\n",
      "iter 8000, loss: 1.2662583674244303      \n",
      "----\n",
      " other):\n",
      "                                                                                                                                                                                                 \n",
      "----\n",
      "iter 9000, loss: 1.1787136197742145      \n",
      "----\n",
      " __(grackward(self, other):\n",
      "                                                                                                                                                                              \n",
      "----\n",
      "iter 10000, loss: 1.1024118488294417      \n",
      "----\n",
      " f *                                                                                                                                                                                                      \n",
      "----\n",
      "iter 11000, loss: 1.0358320833439534      \n",
      "----\n",
      " def __rself *   def __red(self *                                                         def __red(self, other):\n",
      "                                                         def __rif buel):\n",
      "           de \n",
      "----\n",
      "iter 12000, loss: 0.9861815137592848      \n",
      "----\n",
      " ef __red ackwalde+*out  other):\n",
      "                                                                                                                                                                         \n",
      "----\n",
      "iter 13000, loss: 0.943784027168462       \n",
      "----\n",
      " = other * self.data  other =                                                                                                                                                                             \n",
      "----\n",
      "iter 14000, loss: 0.9073598088400529      \n",
      "----\n",
      " ata: fother):\n",
      "                                                                                      def __ne self, other):\n",
      "                                                                              \n",
      "----\n",
      "iter 15000, loss: 0.8671118303725012      \n",
      "----\n",
      " alue):\n",
      "                                                                                                                                                                    def __(self, other),\n",
      "         \n",
      "----\n",
      "iter 16000, loss: 0.8369713587436901      \n",
      "----\n",
      " ut\n",
      "\n",
      "                                                                                                        def _backward():\n",
      "                              out.datackward(), '*')\n",
      "\n",
      "                      \n",
      "----\n",
      "iter 17000, loss: 0.815768482387308       \n",
      "----\n",
      "     other.grad)(selsed = other * self, other):\n",
      "                     out\n",
      "   other.grad = av) angdatackwar__(other):\n",
      "                             out._backward = _backward\n",
      "\n",
      "                         def  \n",
      "----\n",
      "iter 18000, loss: 0.7990838408754943      \n",
      "----\n",
      " __((other):\n",
      "           self.datatadd__(self, other):\n",
      "         return self * other):\n",
      "           self.data v(self, -o__red)\n",
      "             other ind = other.datackward\n",
      "\n",
      "        out._backward (self *dengba \n",
      "----\n",
      "iter 19000, loss: 0.7820239135038652      \n",
      "----\n",
      " f**-1\n",
      "\n",
      "                                                                                                                                                                  def build_tangba, other):\n",
      "      \n",
      "----\n",
      "iter 20000, loss: 0.7606993661180469      \n",
      "----\n",
      " alue out.grad += other + other, other)= other, other)\n",
      "          ov if _backward = a out.grad += other, other):\n",
      "                                                                 other.grad\n",
      "         othe \n",
      "----\n",
      "iter 21000, loss: 0.7521923991784364      \n",
      "----\n",
      "  other if is                       self.grad = ad = ief __ntop = out.grad\n",
      "                                                          dinceself * other = self.data + out._backward = _backward\n",
      "\n",
      "          \n",
      "----\n",
      "iter 22000, loss: 0.7337881535239598      \n",
      "----\n",
      "  intbackward\n",
      "\n",
      "          self):\n",
      "          return self, other)\n",
      "    def __rt ilf _backward\n",
      "\n",
      "         self.grad += out._backward a v(self, other):\n",
      "         return other.datadd__(self, other):\n",
      "          se \n",
      "----\n",
      "iter 23000, loss: 0.7163825212334716      \n",
      "----\n",
      " ):\n",
      "         out\n",
      "   other.grad\n",
      "       other.grad\n",
      "        out._backward = self.data ifl (self.datad += other.datad += self.datackward = _backward\n",
      "\n",
      "        out.grad\n",
      "        other.grad\n",
      "         out._backw \n",
      "----\n",
      "iter 24000, loss: 0.7054599441308702      \n",
      "----\n",
      " elf.grad += other.data d(self, other):\n",
      "                                  out._prelself * other):\n",
      "                                    other.grad\n",
      "         out._backward = self.dat = out.grad += (other i \n",
      "----\n",
      "iter 25000, loss: 0.7012374612106623      \n",
      "----\n",
      " atata 0 elf, _backward()\n",
      "\n",
      "               self._backward(self, other):\n",
      "               return self.data < 0 in =her + other = Value(self, other):\n",
      "                 return setopo[]\n",
      "        return self.dat \n",
      "----\n",
      "iter 26000, loss: 0.6905939782579857      \n",
      "----\n",
      "                                                                                                                                                                                                          \n",
      "----\n",
      "iter 27000, loss: 0.6767097939133611      \n",
      "----\n",
      " (self):\n",
      "            other):\n",
      "            other):\n",
      "           out._backward()\n",
      "\n",
      "            other):\n",
      "            out._backward()\n",
      "\n",
      "            other, Value(other)\n",
      "           irn ief _backward()\n",
      "\n",
      "         ot \n",
      "----\n",
      "iter 28000, loss: 0.6715523279023443      \n",
      "----\n",
      " elf):\n",
      "                  visiteredata * other)\n",
      "\n",
      "              visited = self * other =  other):\n",
      "             self._backward\n",
      "\n",
      "                    r_datadd__(seluiln vadd__(self, other):\n",
      "              se \n",
      "----\n",
      "iter 29000, loss: 0.6487494064730406      \n",
      "----\n",
      " pother + other)\n",
      "\n",
      "          viv__pr__(self,), 'ReLU')\n",
      "\n",
      "            vif* -1\n",
      "\n",
      "              self.datad sedeflf + _backward(self, other):\n",
      "            vif* -1\n",
      "\n",
      "              self.grad += (self, other):\n",
      "    \n",
      "----\n",
      "iter 30000, loss: 0.6336900526071075      \n",
      "----\n",
      " ref _backward():\n",
      "        out._backward = _backward():\n",
      "            self.grad += (out\n",
      "\n",
      "            out\n",
      "\n",
      "        other.grad += out.grad += (other)\n",
      "        out._backward\n",
      "\n",
      "        other.grad += (other.grad \n",
      "----\n",
      "iter 31000, loss: 0.618113159491731       \n",
      "----\n",
      " ata * out.grad += out.grad\n",
      "        ot  isin v__(self):\n",
      "            other.grad += other.grad += other if  other + other) a = = other int opowad = out\n",
      "\n",
      "            other instance(other)\n",
      "        return o \n",
      "----\n",
      "iter 32000, loss: 0.6102691018273839      \n",
      "----\n",
      " ta  Value(self, other * self.data, f1kw\n",
      "\n",
      "        self.grad\n",
      "        def __ren):\n",
      "            self.grad = 0\n",
      "\n",
      "        self.grad\n",
      "        self.grad += other * other * other, Value(self, other * self.data +  \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 33000, loss: 0.5910133543378255      \n",
      "----\n",
      " ifl_(self, other):\n",
      "            f   self.grad += (self.data * other.grad += (other = other if  out._backward():\n",
      "        return self.data * other.grad += (other.grad v)\n",
      "    \"\"\"        self.data * out\n",
      "\n",
      "  \n",
      "----\n",
      "iter 34000, loss: 0.5892072288067064      \n",
      "----\n",
      " ckward()\n",
      "        other + out = Value(self.data, (self.data * other * self._backward = _backward = _backward\n",
      "\n",
      "        out._backward\n",
      "\n",
      "        self.grad += other.grad += other * self.data < 0\n",
      "\n",
      "        ou \n",
      "----\n",
      "iter 35000, loss: 0.5916507806410812      \n",
      "----\n",
      " er if  other int ata * other.data, (self, other):\n",
      "        out._backward\n",
      "\n",
      "        self.data * other if  out._backward = _backward = _backward\n",
      "\n",
      "        return out.grad\n",
      "        other + (-out.grad = = a s \n",
      "----\n",
      "iter 36000, loss: 0.5827917270633604      \n",
      "----\n",
      " ladd(self, other):\n",
      "        return self * other.grad\n",
      "        self.grad += other + other + (-other):\n",
      "    def __rtrelf.data + other.datadd__(v)\n",
      "            self.grad += other + other):\n",
      "        return sel \n",
      "----\n",
      "iter 37000, loss: 0.5738083751529428      \n",
      "----\n",
      "    def _backward\n",
      "\n",
      "            ou n +ance(self.data * other):\n",
      "            def _backward = _backward():\n",
      "                self.grad += other if  other.grad += _balue other):\n",
      "                out._backward( \n",
      "----\n",
      "iter 38000, loss: 0.5700953148049319      \n",
      "----\n",
      "         out._backward():\n",
      "                out._backwardind0)\n",
      "            self._rad += other.data, (owat = 0 if  other = self * other if  other.grad += out.grad += other if  other.grad += out.grad\n",
      "      \n",
      "----\n",
      "iter 39000, loss: 0.5650389239248612      \n",
      "----\n",
      "           = += self.data * out\n",
      "\n",
      "    def __(self, other):\n",
      "        return other * self.data < 0 int ata, (self, other):\n",
      "        return self, other, Value(self, other):\n",
      "       if *out.grad\n",
      "\n",
      "    def __rta \n",
      "----\n",
      "iter 40000, loss: 0.5617882917047011      \n",
      "----\n",
      "          self._backward = _backward()\n",
      "\n",
      "    def __rtorerevadd=\n",
      "\n",
      "    def __rtopo(self):\n",
      "        return self.data * other + (-self, other):\n",
      "        self.grad += (self * other + other + (-topo):\n",
      "        r \n",
      "----\n",
      "iter 41000, loss: 0.5595253545914035      \n",
      "----\n",
      " f  other * self.data * out.grad += other * self.data * out.grad\n",
      "        out._backward():\n",
      "            self.grad += (out.grad += (other)\n",
      "\n",
      "            out._backward():\n",
      "            out = Value Value) if.g \n",
      "----\n",
      "iter 42000, loss: 0.5562133124918192      \n",
      "----\n",
      " ue Value(self * other = other.data   init__(self, other):\n",
      "            out   other * self.data  other if  other = out._backward = ) eseg__(self, other):\n",
      "            self.grad\n",
      "        self.__re other if \n",
      "----\n",
      "iter 43000, loss: 0.5561774764203368      \n",
      "----\n",
      " ssstrederedef _backward():\n",
      "        return self, other + other if  other * self.data + other + other if  out._backward():\n",
      "        return self, other + other + other\n",
      "\n",
      "    def __t(int ata= Value Value(se \n",
      "----\n",
      "iter 44000, loss: 0.55644560736072        \n",
      "----\n",
      " \n",
      "            return (itaddackward():\n",
      "                                                              self.grad +ad = s if  other * self.grad += (out._backward()\n",
      "\n",
      "    def __rtother + other.data, (self):\n",
      " \n",
      "----\n",
      "iter 45000, loss: 0.5538215668217837      \n",
      "----\n",
      " ata, (v) ) ed:\n",
      "            self.grad +a senstance(out.grad\n",
      "        out._backward = _backward = _backward = _backward\n",
      "\n",
      "    def __renother)\n",
      "        self.grad\n",
      "        out = Value) else Value(other, Value \n",
      "----\n",
      "iter 46000, loss: 0.5563016243969         \n",
      "----\n",
      "       seturn self.data={self._backward = _prev\n",
      "\n",
      "    def __(self, other)\n",
      "\n",
      "    def __renul__(self, other):\n",
      "            fher = +'):\n",
      "            self._backward = _backward = _backward = _backward\n",
      "\n",
      "    def \n",
      "----\n",
      "iter 47000, loss: 0.5477316652510976      \n",
      "----\n",
      " if      v f), '):\n",
      "          f v-1)\n",
      "        return self.data * other + other * self.data < 0 if  out\n",
      "\n",
      "    def __rstelf):\n",
      "                            self.grad += other * self.grad = 1\n",
      "        self._bac \n",
      "----\n",
      "iter 48000, loss: 0.5442704196428455      \n",
      "----\n",
      " ):\n",
      "            self.grad += other, (self, other):\n",
      "                    on 0            out(r):\n",
      "                other + out.data, (self):\n",
      "            out   other.grad += (out.data > 0) * out.grad += oth \n",
      "----\n",
      "iter 49000, loss: 0.5382736834676239      \n",
      "----\n",
      " ce):   other.grad +in def ___(self, other)\n",
      "\n",
      "    def __renevar = other)\n",
      "        out._backward\n",
      "\n",
      "            self.grad += other.data * out.grad += other * self._backward = _backward = _backward\n",
      "\n",
      "    def  \n",
      "----\n",
      "iter 50000, loss: 0.5381759981239412      \n",
      "----\n",
      "        self._backward\n",
      "\n",
      "    def __rtornd\n",
      "\n",
      "    def __(strnd += (-self.grad = 0 if  other.data  =       self._backward = _backward = _backward():\n",
      "        topher.data * out.grad += other * self._backward\n",
      " \n",
      "----\n",
      "iter 51000, loss: 0.5286987463932252      \n",
      "----\n",
      " d          def _backward(self)\n",
      "\n",
      "\n",
      "        def __redef _backward(self)\n",
      "\n",
      "    def __rst(self)\n",
      "                      []\n",
      "        topoitadda=), fow\"\n",
      "        out._backward = _badkward\n",
      "\n",
      "        return self * o \n",
      "----\n",
      "iter 52000, loss: 0.5199737003736659      \n",
      "----\n",
      " __(self, other):\n",
      "        return out._backward = _backward\n",
      "\n",
      "        self._preladdackward():\n",
      "        return out\n",
      "\n",
      "            self.grad\n",
      "        out._backward\n",
      "\n",
      "            self.grad += out.grad = 1\n",
      "       \n",
      "----\n",
      "iter 53000, loss: 0.5151202956958284      \n",
      "----\n",
      " dd__(self, other * other.data * out.grad\n",
      "            other.data, (self, other):\n",
      "        return self * other.data)\n",
      "            vita, (self, other):\n",
      "            other):\n",
      "        return other.data  = othe \n",
      "----\n",
      "iter 54000, loss: 0.512170646058798       \n",
      "----\n",
      " {o_had\n",
      "\n",
      "    def __ngpsechatkward():\n",
      "        return self + other\n",
      "\n",
      "    def __rt(seburd(self):\n",
      "        return self * other + ot         self._backward():\n",
      "            self.grad +=  \"\"\"\"      returd = n(se \n",
      "----\n",
      "iter 55000, loss: 0.5053467303475833      \n",
      "----\n",
      " nelf.data, (self, other), '*')\n",
      "\n",
      "                                               out = Value(self * other = out._backward = _backward = _backward\n",
      "\n",
      "                                                        \n",
      "----\n",
      "iter 56000, loss: 0.5047014479711348      \n",
      "----\n",
      " \n",
      "            out._backward = _backward = s nlse(self, other):\n",
      "            self.grad += other)\n",
      "        def _backward():\n",
      "            self.grad += out.grad\n",
      "        out._backward ata  other):\n",
      "        othe \n",
      "----\n",
      "iter 56380, loss: 0.5006902646897615      "
     ]
    }
   ],
   "source": [
    "import sys        \n",
    "\n",
    "def train():\n",
    "    n, p, hs = 0, 0, None\n",
    "    adam = AdamW(rnn.parameters())\n",
    "    \n",
    "    print(f'\\n----\\n {sample_text(None, data[0], 200)} \\n----')\n",
    "    \n",
    "    smooth_loss = -math.log(1.0/vocab_size) # loss at iteration 0\n",
    "    while smooth_loss > 0.5: # ~40-200k iterations depending on how intently you watch it\n",
    "        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "        if p+seq_length+1 >= len(data) or n == 0:\n",
    "            p, hs = random.randint(0, seq_length-1), None # go from start of data\n",
    "        inputs, targets = data_char_ixs[p:p+seq_length], data_char_ixs[p+1:p+seq_length+1]\n",
    "\n",
    "        # forward seq_length characters through the net and fetch gradient\n",
    "        rnn.zero_grad()\n",
    "        loss, hs = calc_loss(inputs, targets, hs)\n",
    "        loss.backward()\n",
    "        for param in rnn.parameters():\n",
    "            param.grad = max(-5., min(5., param.grad)) # clip to mitigate exploding gradients\n",
    "        smooth_loss = lerp(smooth_loss, loss.data, 0.001)\n",
    "        adam.update_params()\n",
    "        \n",
    "        if (n+1) % 10 == 0:\n",
    "            sys.stdout.write(f'\\riter {n+1}, loss: {smooth_loss}      ') # print progress\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # sample from the model now and then\n",
    "        if (n+1) % 1000 == 0:\n",
    "            print(f'\\n----\\n {sample_text(hs, ix_to_char[inputs[0]], 200)} \\n----')\n",
    "\n",
    "        p += seq_length # move data pointer\n",
    "        n += 1 # iteration counter\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd870af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      " \n",
      "\n",
      "        def _backward():\n",
      "            self.grad += (opopow\"\n",
      "            out = Value(self.data, (self, other):\n",
      "        return self.data, (self, other):\n",
      "        def _backward():\n",
      "            out(r):\n",
      "\n",
      "            out = Value(self.data * out.grad\n",
      "        out._backward = _backward = _backward():\n",
      "            bui d nchildrat ='):\n",
      "        out._backward\n",
      "\n",
      "        other.grad\n",
      "        out = Value(0 instar* other + other.data, (self, other):\n",
      "        out = Value(0 nckward\n",
      "\n",
      "        self._backward = _backward = _backward\n",
      "\n",
      "        other.grad\n",
      "        out._backward = _backward\n",
      "\n",
      "        return out._backward = _backward\n",
      "\n",
      "            out\n",
      "\n",
      "        return out\n",
      "\n",
      "            out = Value(self.data, _topo(out\n",
      "\n",
      "    def __ren):\n",
      "            out = Value(self.data * out.grad\n",
      "        out._backward = _backward = _backward = _pself):\n",
      "        out._backward = _backward = _backward\n",
      "\n",
      "        return other.data * out.grad\n",
      "            out = Value(self.data * out.grad += out.grad\n",
      "        self.data * out.grad\n",
      "        out._backward = _backward():\n",
      "            self.grad += other.data * out.grad\n",
      "        out._backward\n",
      "\n",
      "        self._prev iin(self, other):\n",
      "        out = Value(self.data * out.grad\n",
      "        out._backward at ='):\n",
      "        return other):\n",
      "        out = Value(self.data * out.grad\n",
      "        self._backward\n",
      "\n",
      "        other.grad += out.grad\n",
      "        out = Value(self.data * out.grad\n",
      "        out._backward\n",
      "\n",
      "        def __topo)\n",
      " f * other, (self.data, (self, other):\n",
      "        self.grad += out.grad\n",
      "        out._backward\n",
      "\n",
      "        other.data, (self, other)\n",
      "\n",
      "        other * other.grad\n",
      "        self.grad += out.grad\n",
      "        out._backward = _backward = s nlsengrad\n",
      "        out._badkward = _backward():\n",
      "                    out = Value(self.data, (self, other)\n",
      "        out._backward\n",
      "\n",
      "        return out\n",
      "\n",
      "            self.grad\n",
      "        out = Value(self.data * out.grad += out.grad\n",
      "        out._backward = _backward\n",
      "\n",
      "        other.grad\n",
      "        out = Value(self.data, (self, other), '*')\n",
      "\n",
      "        self.data, (self, other):\n",
      "            other.dat \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# We've finally finished training; let's sample a big chunk of text!\n",
    "print(f'\\n----\\n {data[0] + sample_text(None, data[0], 2000)} \\n----')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
